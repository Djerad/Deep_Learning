Instead of processing pixels one by one, ViT splits an image into a fixed grid of small patches (e.g. 16×16 pixels each). Each patch is flattened and linearly projected into an embedding vector, treating it the same way a word token is treated in NLP. A special learnable [CLS] token is prepended to the sequence, and positional embeddings are added so the model knows where each patch came from.
This sequence of tokens is then passed through a stack of standard Transformer encoder blocks. Each block runs multi-head self-attention — letting every patch look at every other patch globally — followed by a feed-forward MLP. Unlike CNNs, there's no local inductive bias; the model learns spatial relationships entirely from data.
After all the encoder blocks, the output at the [CLS] token position captures a global representation of the image. A simple linear layer maps it to class logits for classification.
The key insight is that self-attention has no notion of distance, so a patch in the top-left corner can directly attend to one in the bottom-right from layer one. This makes ViT extremely powerful at capturing long-range dependencies, though it typically requires large datasets or pretraining to outperform CNNs, since it lacks the built-in spatial priors that convolutions provide.
